{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tkinter as tk\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = 40\n",
    "maze_h = 8\n",
    "maze_w = 8\n",
    "episode_count = 200\n",
    "episodes = range(episode_count)\n",
    "rewards = []\n",
    "movements = []\n",
    "reward_black = -100\n",
    "reward_goal = 100\n",
    "reward_normal = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    def __init__(self):\n",
    "        self.window = tk.Tk()\n",
    "        self.window.title(\"Maze with Q-Learning\")\n",
    "        self.window.geometry('{0}x{1}'.format(maze_w*unit,maze_h*unit))\n",
    "        self.action_space = {'u','d','r','l'}\n",
    "        self.n_action = len(self.action_space)\n",
    "        self.holes = []\n",
    "        self.build_maze()\n",
    "        \n",
    "    def build_maze(self):\n",
    "        self.canvas = tk.Canvas(self.window,bg='white',width=maze_w*unit,height=maze_h*unit)\n",
    "        \n",
    "        for c in range(0,maze_w*unit,unit):\n",
    "            x0, y0, x1, y1 = c ,0 ,c ,maze_w*unit\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "        \n",
    "        for r in range(0,maze_h*unit,unit):\n",
    "            x0, y0, x1, y1 = 0, r, maze_h*unit, r\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "            \n",
    "        origin = np.array([unit/2,unit/2])\n",
    "        \n",
    "        hole_offset =[[0,1], \n",
    "              [1,1], [1,2], [1,4], [1,5], [1,6], [1,7], \n",
    "              [2,1], [2,6], \n",
    "              [3,3], [3,4], [3,6], \n",
    "              [4,1], [4,3], \n",
    "              [5,0], [5,1], [5,3], [5,5], [5,7],\n",
    "              [6,1], [6,2], [6,3], [6,5],\n",
    "              [7,5], [7,6]]\n",
    "        for i in range(len(hole_offset)):\n",
    "            s = \"hole\"+str(i)\n",
    "            hole_center = origin + np.array([unit*hole_offset[i][1],unit*hole_offset[i][0]])\n",
    "            self.s = self.canvas.create_rectangle(\n",
    "                hole_center[0] - 15, hole_center[1] - 15,\n",
    "                hole_center[0] + 15, hole_center[1] + 15,\n",
    "                fill=\"black\"\n",
    "            )\n",
    "            self.holes.append(self.canvas.coords(self.s))\n",
    "        \n",
    "        goal_center = origin + np.array([unit*7,unit*7])\n",
    "        self.goal = self.canvas.create_rectangle(\n",
    "            goal_center[0] - 15, goal_center[1] - 15,\n",
    "            goal_center[0] + 15, goal_center[1] + 15,\n",
    "            fill=\"yellow\"\n",
    "        )\n",
    "        \n",
    "        self.sprite = self.canvas.create_rectangle(\n",
    "            origin[0] - 15, origin[1] - 15,\n",
    "            origin[0] + 15, origin[1] + 15,\n",
    "            fill=\"red\"\n",
    "        )\n",
    "        \n",
    "        self.canvas.pack()\n",
    "        \n",
    "    def render(self):\n",
    "        time.sleep(0.1)\n",
    "        self.window.update()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.window.update()\n",
    "        time.sleep(0.5)\n",
    "        self.canvas.delete(self.sprite)\n",
    "        origin = np.array([unit/2,unit/2])\n",
    "        self.sprite = self.canvas.create_rectangle(\n",
    "            origin[0] - 15, origin[1] - 15,\n",
    "            origin[0] + 15, origin[1] + 15,\n",
    "            fill=\"red\"\n",
    "        )\n",
    "        return self.canvas.coords(self.sprite)\n",
    "        \n",
    "    def get_state_reward(self, action):\n",
    "        s = self.canvas.coords(self.sprite)\n",
    "        base_action = np.array([0,0])\n",
    "        if action == 0: #up\n",
    "            if s[1] > unit:\n",
    "                base_action[1] -= unit\n",
    "        elif action == 1: #down\n",
    "            if s[1] < (maze_h - 1)*unit:\n",
    "                base_action[1] += unit\n",
    "        elif action == 2: #right\n",
    "            if s[0] < (maze_w - 1)*unit:\n",
    "                base_action[0] += unit\n",
    "        elif action == 3: #left\n",
    "            if s[0] > unit:\n",
    "                base_action[0] -= unit\n",
    "            \n",
    "        self.canvas.move(self.sprite, base_action[0], base_action[1])\n",
    "        s_  = self.canvas.coords(self.sprite)\n",
    "        if s_ == self.canvas.coords(self.goal):\n",
    "            reward = reward_goal\n",
    "            done = True\n",
    "            s_ = 'terminal'\n",
    "        elif s_ in self.holes:\n",
    "            reward = reward_black\n",
    "            done = True\n",
    "            s_ = 'terminal'\n",
    "        else:\n",
    "            reward = reward_normal\n",
    "            done = False\n",
    "        return s_, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.03, reward_decay=0.9, e_greedy=0.1, train=1):\n",
    "        self.actions = actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        try:\n",
    "            self.q_table = pd.read_csv(r'RL_Maze_Q_Table.csv',index_col=0)\n",
    "            self.q_table = self.q_table.rename(index=str,columns={'0':0, '1':1, '2':2, '3':3}) #change column to type int to avoid errors\n",
    "        except FileNotFoundError:\n",
    "            self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "        self.train = train\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        self.add_state(observation)\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            action = state_action.idxmax()\n",
    "        return action\n",
    "    \n",
    "    def learn(self, s, a, r, s_):\n",
    "        if self.train == 1:\n",
    "            self.add_state(s_)\n",
    "            q_predict = self.q_table.loc[s,a]\n",
    "            if s_ != 'terminal':\n",
    "                q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "            else:\n",
    "                q_target = r\n",
    "            self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "            self.q_table.to_csv(r'RL_Maze_Q_Table.csv',index=True)\n",
    "    \n",
    "    def add_state(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions),\n",
    "                         index=self.q_table.columns,\n",
    "                         name=state)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    for episode in episodes:\n",
    "        print(\"Episode {0}/{1}\".format(episode, episode_count))\n",
    "        observation = env.reset()\n",
    "        moves = 0\n",
    "        reward = 0\n",
    "        reached = []\n",
    "        reached.append(observation)\n",
    "        \n",
    "        while True:\n",
    "            env.render()\n",
    "            action = q_learning_agent.choose_action(str(observation))\n",
    "            observation_, reward_, done = env.get_state_reward(action)\n",
    "            moves += 1\n",
    "            if observation_ not in reached:\n",
    "                reward += reward_\n",
    "                reached.append(observation_)\n",
    "                if reward_ == reward_goal:\n",
    "                    print(\"Goal reached\")\n",
    "            else:\n",
    "                reward -= (reward_)*5\n",
    "            \n",
    "            q_learning_agent.learn(str(observation), action, reward, str(observation_))\n",
    "            observation = observation_\n",
    "            \n",
    "            if done:\n",
    "                movements.append(moves)\n",
    "                rewards.append(reward)\n",
    "                print(\"Reward: {0}, Moves: {1}\".format(reward, moves))\n",
    "                break\n",
    "    print(\"game over!\")\n",
    "    plot_reward_movements()\n",
    "\n",
    "def plot_reward_movements():\n",
    "    plt.figure()\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(episodes, movements)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"#Movements\")\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.step(episodes, rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 0 for training and 1 for evaluation => 0\n",
      "Enter number of training epochs20\n",
      "Episode 0/20\n",
      "Goal reached\n",
      "Reward: 105, Moves: 18\n",
      "Episode 1/20\n",
      "Goal reached\n",
      "Reward: 115, Moves: 16\n",
      "Episode 2/20\n",
      "Reward: -100, Moves: 1\n",
      "Episode 3/20\n",
      "Reward: -89, Moves: 12\n",
      "Episode 4/20\n",
      "Goal reached\n",
      "Reward: 105, Moves: 18\n",
      "Episode 5/20\n",
      "Goal reached\n",
      "Reward: 115, Moves: 16\n",
      "Episode 6/20\n",
      "Reward: -88, Moves: 13\n",
      "Episode 7/20\n",
      "Goal reached\n",
      "Reward: 105, Moves: 18\n",
      "Episode 8/20\n",
      "Goal reached\n",
      "Reward: 115, Moves: 16\n",
      "Episode 9/20\n",
      "Goal reached\n",
      "Reward: 115, Moves: 16\n",
      "Episode 10/20\n",
      "Goal reached\n",
      "Reward: 105, Moves: 18\n",
      "Episode 11/20\n",
      "Reward: -94, Moves: 7\n",
      "Episode 12/20\n",
      "Goal reached\n",
      "Reward: 115, Moves: 16\n",
      "Episode 13/20\n",
      "Reward: -96, Moves: 11\n",
      "Episode 14/20\n",
      "Goal reached\n",
      "Reward: 115, Moves: 16\n",
      "Episode 15/20\n",
      "Goal reached\n",
      "Reward: 111, Moves: 18\n",
      "Episode 16/20\n",
      "Goal reached\n",
      "Reward: 115, Moves: 16\n",
      "Episode 17/20\n",
      "Goal reached\n",
      "Reward: 115, Moves: 16\n",
      "Episode 18/20\n",
      "Reward: -91, Moves: 10\n",
      "Episode 19/20\n",
      "Goal reached\n",
      "Reward: 115, Moves: 16\n",
      "game over!\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    choice = int(input(\"Enter 0 for training and 1 for evaluation => \"))\n",
    "    if choice == 0:\n",
    "        episode_count = int(input(\"Enter number of training epochs\"))\n",
    "        episodes = range(episode_count)\n",
    "        env = Maze()\n",
    "        q_learning_agent = QLearningTable(actions=list(range(env.n_action)))\n",
    "        env.window.after(10, run_experiment)\n",
    "        env.window.mainloop()\n",
    "    elif choice == 1:\n",
    "        episode_count = 10\n",
    "        episodes = range(episode_count)\n",
    "        env = Maze()\n",
    "        q_learning_agent = QLearningTable(e_greedy=0.0, train=0, actions=list(range(env.n_action)))\n",
    "        env.window.after(10, run_experiment)\n",
    "        env.window.mainloop()\n",
    "    else:\n",
    "        print(\"Wrong choice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
